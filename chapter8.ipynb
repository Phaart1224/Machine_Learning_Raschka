{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a1b7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "if not os.path.isdir('aclImdb'):\n",
    "\n",
    "    with tarfile.open('aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
    "        tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2af65a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:09\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "basepath = 'aclImdb'\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "data = []\n",
    "\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file),\n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            data.append([txt, labels[l]])\n",
    "            pbar.update()\n",
    "\n",
    "df = pd.DataFrame(data, columns=['review', 'sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6cab974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>at a Saturday matinee in my home town. I went ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love this movie. It is the first film Master...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the voice over which begins the film, Hughi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  at a Saturday matinee in my home town. I went ...          0\n",
       "1  I love this movie. It is the first film Master...          1\n",
       "2  In the voice over which begins the film, Hughi...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9396cd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "### 본격적인 BoW 이해해보기\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 사이킷런에는 역시 BoW를 바로 사용해볼 수 있는 모델이 존재함.\n",
    "count = CountVectorizer()\n",
    "docs = np.array(['The sun is shining',\n",
    "                'The weather is sweet',\n",
    "                'The sun is shining, the weather is sweet and one and one is two'])\n",
    "# 임시 문장을 만들어주고 count의 fit_transform 메서드를 통해서 단어 사전을 만들 수 있음.\n",
    "bag = count.fit_transform(docs)\n",
    "print(count.vocabulary_)\n",
    "\n",
    "# 여기서 딕셔너리의 value는 단어의 고유한 인덱스임.\n",
    "# 밑 셀의 예시를 보면 쉽게 이해할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a7b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())\n",
    "\n",
    "# 예를 들어 1번째 문장을 보면, 1번 인덱스인 is가 1번 포함됐으니 1로 되어있고, 4번 인덱스인 sun이 포함되어있으니 1로 되어있고\n",
    "# 이런식으로 인덱스별로 매칭되어있음.\n",
    "# 3번째 문장은 is가 총 3번 포함되어 있으니 1번 인덱스가 3으로 되어있는 모습.\n",
    "\n",
    "\n",
    "### 참고로 이번에 단어 사전을 만들때 1단어 단위(1-gram)로 끊어서 그렇지 n-gram 단위로 끊어서 단어사전을 만들 수 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a07b171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "### TF-IDF로 특성 벡터에서 단어의 가중치를 낮춰보기\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# L2 norm을 사용하여 정규화\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())\n",
    "\n",
    "# tf-idf를 사용하면 is 같은 문장의 결정적인 요소가 아니지만 쓸데없이 높게 빈도수가 잡히는걸 줄여줄 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa78750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        at a saturday matinee in my home town i went w...\n",
       "1        i love this movie it is the first film master ...\n",
       "2        in the voice over which begins the film hughie...\n",
       "3         spoiler alert the point is though that i didn...\n",
       "4        this is an excellent film no it s not mel gibs...\n",
       "                               ...                        \n",
       "49995    although the director tried the filming was ma...\n",
       "49996    it has been about 50 years since a movie has b...\n",
       "49997     bar hopping seems to be trying to be about th...\n",
       "49998    this awful effort just goes to show what happe...\n",
       "49999    yes why among the filmmakers that came out in ...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 텍스트 데이터 정제\n",
    "import re\n",
    "\n",
    "df.loc[0, 'review'][-50:]\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "preprocessor(df.loc[0, 'review'][-50:])\n",
    "\n",
    "df['review'] = df['review'].apply(preprocessor)\n",
    "df['review'].map(preprocessor)\n",
    "\n",
    "# 데이터셋이 HTML에서 가져온 문장이 있다보니 tag가 있는 리뷰가 있어서 그걸 없애기 위해서\n",
    "# 전처리 함수를 통해 dataframe의 모든 문장을 전처리\n",
    "# 정규 표현식인 Regular Expression(re) 라이브러리를 가져와서 쉽게 전처리 할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514c768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 문서를 토큰 단위로 나누기\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "tokenizer('runners like running and thus they run')\n",
    "\n",
    "# 정말 단순하게 1줄짜리 코드로 단어를 나눌 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95bd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 토큰을 어간형태로 뽑아주는 어간추출 만들기\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "tokenizer_porter('runners like running and thus they run')\n",
    "\n",
    "# 이런식으로 단어의 원형으로 뽑아낼 수 있음.\n",
    "# 하지만, 어간 추출은 실제로 사용하지 않는 단어들을 뽑아 낼 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aedf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\조승현\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 불용어 제거\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]\n",
    "\n",
    "# 불용어는 모든 종류의 텍스트에 아주 흔하게 단어. 관사나, 동사중에서 매우 자주쓰이는 단어들 그런걸 불용어라고함.\n",
    "# 그런건 사실상 감성분석을 할때 의미 없는 단어들인데 빈도수만 높게 잡히기 때문에 그냥 없애버리는 불용어 제거 기법을 사용함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ccd6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서분류를 위한 로지스틱 회귀\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
